---
title: "Big Data Analytics"
author: "siju.swamy@saintgits.org"
date: "`r Sys.Date()`"
output:
  html_document: default
    fig_width: 12 
    fig_height: 8
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE,message=FALSE)
knitr::opts_chunk$set(fig.width=12, fig.height=8) 
```

```{r}
df=read.csv("https://raw.githubusercontent.com/sijuswamy/B-Data-Analytics-Workshop-/main/mycsvfile.csv")
#df=read.csv("mycsvfile.csv",header=T)
```

```{r}
head(df)
```

```{r}
library(ggplot2)
library(plotly)
library(dplyr)
```

## Ground Fault

```{r}
Pie_Data<-count(df,G)
Pie_Data
```

```{r}
fig <- plot_ly(Pie_Data,values=~n,labels=~factor(G), textinfo='label+percent',
               insidetextorientation='radial',marker=list(colors=c("green","red")),type='pie')
fig
```


## Line A fault

```{r}
Pie_Data<-dplyr::count(df,A)
Pie_Data
```

```{r}
fig <- plot_ly(Pie_Data,values=~n,labels=~factor(A), textinfo='label+percent',
               insidetextorientation='radial',marker=list(colors=c("blue","green")),type='pie')
fig
```


## Coversion of the data to long format

```{r}
library(reshape2)
library(dplyr)
data_long <- melt(df,
        # ID variables - all the variables to keep but not split apart on
    id.vars=c("Ia","Ib","Ic","Va","Vb","Vc"),
        # The source columns
    measure.vars=c("G", "C", "B","A"),
        # Name of the destination column that will identify the original
        # column that the measurement came from
    variable.name="Fault_Location",
    value.name="Fault"
)
#data_long <- arrange(data_long,data_long$Fault_Location, data_long$Fault)# to get a sorted view
#data_long
```


```{r}
#gender-wise
data_long$Fault=as.factor(data_long$Fault)
library(ggplot2)
data_long%>%
  count(Fault_Location,Fault) %>%       
  group_by(Fault_Location) %>%
  mutate(pct= prop.table(n) * 100) %>%
  ggplot() + aes(Fault_Location, pct, fill=Fault) +
  geom_bar(stat="identity") +
  ylab("Number of respondents") +
  geom_text(aes(label=paste0(sprintf("%1.1f", pct),"%")),
            position=position_stack(vjust=0.5)) +labs(x ="Fault Location", y = "Percentage",fill="Fault")+
  theme_bw()
```

## Creating new variable to represents fault type

```{r}
df$Ftype=paste0(as.character(df$G),as.character(df$C),as.character(df$B),as.character(df$A))
df$Fault_Cat <- plyr::mapvalues(df$Ftype, from = c("0000", "1001","0110","1011","0111","1111"), to = c('NO Fault', 'Line A to Ground Fault','Line B to Line C Fault','Line A Line B to Ground Fault','Line A Line B Line C','Line A Line B Line C to Ground Fault'))
```
```{r}
d1=df[,c(-1,-2,-3,-4)]
head(d1)
```
```{r}
summary(d1)
```

```{r}
count(d1,Fault_Cat)
```

```{r}
Pie_Data<-dplyr::count(df,Fault_Cat)
Pie_Data
```

```{r}
fig <- plot_ly(Pie_Data,values=~n,labels=~factor(Fault_Cat), textinfo='value+percent',
               insidetextorientation='radial',type='pie')
fig
```

![](https://4.bp.blogspot.com/-uVv83087Yow/WkyGq2knzCI/AAAAAAAAEP4/cOHQ87sGhyQeR79lVyJRovAF4HEeT6bQACLcBGAs/s1600/world-find-share-on-giphy.gif.gif)


![](http://empossible.net/wp-content/uploads/2018/03/TLStandingWave_Z1GTZ2_logo.gif)
```{r}
Xval=1:length(d1$Ia)
gfg_plot <- ggplot(d1,aes(Xval)) +  
    geom_line(aes(y = Ia), color = "blue") +
     geom_line(aes(y = Ib), color = "red") +
    geom_line(aes(y = Ic), color = "green")
    gfg_plot
```

```{r}
plot_ly(x = Xval) %>%
  add_lines(y = d1$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = d1$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = d1$Ic, color = I("blue"), name = "Ic")
```

```{r}
plot_ly(x = Xval) %>%
  add_lines(y = d1$Va, color = I("red"), name = "Va") %>%
  add_lines(y = d1$Vb, color = I("green"), name = "Vb")%>%
  add_lines(y = d1$Vc, color = I("blue"), name = "Vc")
```

## Voltage or Current graph, where there is large fluctuation in the graph, there faults have occurred

**Line A**
----

```{r}
p1=ggplot(d1, aes(x=Va)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(d1, aes(x=Ia))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ia", y = "Density")
p3=ggplot(d1, aes(x=Va)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(d1, aes(x=Ia)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

---

**Line B**


```{r}
p1=ggplot(d1, aes(x=Vb)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(d1, aes(x=Ib))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ib", y = "Density")
p3=ggplot(d1, aes(x=Vb)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(d1, aes(x=Ib)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

---

**Line C**


```{r}
p1=ggplot(d1, aes(x=Vc)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(d1, aes(x=Ic))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ic", y = "Density")
p3=ggplot(d1, aes(x=Vc)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(d1, aes(x=Ic)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

## No fault distribution

```{r}
nofault=d1%>%filter(Fault_Cat=="NO Fault")
Xval=1:length(nofault$Ia)
```

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = nofault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = nofault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = nofault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = nofault$Va, color = I("red"), name = "Ia") %>%
  add_lines(y = nofault$Vb, color = I("green"), name = "Ib")%>%
  add_lines(y = nofault$Vc, color = I("blue"), name = "Ic")
```

---

##1. Distribution of `non fault` voltage and Load


**Line A**
----

```{r}
p1=ggplot(nofault, aes(x=Va)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(nofault, aes(x=Ia))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ia", y = "Density")
p3=ggplot(nofault, aes(x=Va)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(nofault, aes(x=Ia)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

---

**Line B**


```{r}
p1=ggplot(nofault, aes(x=Vb)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(nofault, aes(x=Ib))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ib", y = "Density")
p3=ggplot(nofault, aes(x=Vb)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(nofault, aes(x=Ib)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

---

**Line C**


```{r}
p1=ggplot(nofault, aes(x=Vc)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(nofault, aes(x=Ic))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ic", y = "Density")
p3=ggplot(nofault, aes(x=Vc)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(nofault, aes(x=Ic)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

## 2. Faulty System with Line A to Ground Fault

```{r}
Line_AG_Fault=d1%>%filter(Fault_Cat=="Line A to Ground Fault")
Xval=1:length(Line_AG_Fault$Ia)
```

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_AG_Fault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_AG_Fault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_AG_Fault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_AG_Fault$Va, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_AG_Fault$Vb, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_AG_Fault$Vc, color = I("blue"), name = "Ic")
```


*At a time of Line A to ground fault the current in line A increases to 10 fold approximately 1000 Ampears form normal 100 Ampears and voltage reduced.*

---


## 3. Faulty System with Line A ,Line B to Ground Fault

```{r}
Line_ABG_Fault=d1%>%filter(Fault_Cat=="Line A Line B to Ground Fault")
Xval=1:length(Line_ABG_Fault$Ia)
```


```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABG_Fault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_ABG_Fault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_ABG_Fault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABG_Fault$Va, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_ABG_Fault$Vb, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_ABG_Fault$Vc, color = I("blue"), name = "Ic")
```


*At a time of Line A Line B to ground fault the current in line A increases to 10 fold approximately 1000 Ampears form normal 100 Ampears and voltage reduced.*

---

## 4. Faulty System with Line B to Line C



```{r}
Line_BC_Fault=d1%>%filter(Fault_Cat=="Line B to Line C Fault")
Xval=1:length(Line_BC_Fault$Ia)
```

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_BC_Fault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_BC_Fault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_BC_Fault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_BC_Fault$Va, color = I("red"), name = "Va") %>%
  add_lines(y = Line_BC_Fault$Vb, color = I("green"), name = "Vb")%>%
  add_lines(y = Line_BC_Fault$Vc, color = I("blue"), name = "Vc")
```

## 5. Faulty System with Line A - Line B - Line C


```{r}
Line_ABC_Fault=d1%>%filter(Fault_Cat=="Line A Line B Line C")
Xval=1:length(Line_ABC_Fault$Ia)
```

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABC_Fault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_ABC_Fault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_ABC_Fault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABC_Fault$Va, color = I("red"), name = "Va") %>%
  add_lines(y = Line_ABC_Fault$Vb, color = I("green"), name = "Vb")%>%
  add_lines(y = Line_ABC_Fault$Vc, color = I("blue"), name = "Vc")
```

## 6. Faulty System with Line A - Line B - Line C - Ground


```{r}
Line_ABCG_Fault=d1%>%filter(Fault_Cat=="Line A Line B Line C to Ground Fault")
Xval=1:length(Line_ABCG_Fault$Ia)
```

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABCG_Fault$Ia, color = I("red"), name = "Ia") %>%
  add_lines(y = Line_ABCG_Fault$Ib, color = I("green"), name = "Ib")%>%
  add_lines(y = Line_ABCG_Fault$Ic, color = I("blue"), name = "Ic")
```

----

**Voltage**

```{r,fig.width=18, fig.height=3, fig.fullwidth=F}
plot_ly(x = Xval) %>%
  add_lines(y = Line_ABCG_Fault$Va, color = I("red"), name = "Va") %>%
  add_lines(y = Line_ABCG_Fault$Vb, color = I("green"), name = "Vb")%>%
  add_lines(y = Line_ABCG_Fault$Vc, color = I("blue"), name = "Vc")
```

## Distribution of All Line Fault 


```{r}
p1=ggplot(Line_ABCG_Fault, aes(x=Vb)) +
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+labs(title="Distribution of Voltage",x="Voltage ", y = "Density")
p2=ggplot(Line_ABCG_Fault, aes(x=Ib))+
    geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ labs(title="Distribution of Load",x="Ib", y = "Density")
p3=ggplot(Line_ABCG_Fault, aes(x=Vb)) + 
  geom_boxplot(notch=F)#+ coord_flip()
p4=ggplot(Line_ABCG_Fault, aes(x=Ib)) + 
  geom_boxplot(notch=F)#+ coord_flip()
```

```{r}
library(ggpubr)
figure <- ggarrange(p1,p2,p3,p4,
                    labels = c("A", "B", "C","D"),
                    ncol = 2, nrow = 2)
figure
```

---

```{r}

#installing necessary libraries for Machine Learning
#install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth'))
```

## Loading the libraries and Data
```{r}
# Load the caret package
library(caret)

# Import dataset
#df=read.csv("mycsvfile.csv",header=T)
# Structure of the dataframe
str(d1)

# See top 6 rows and 10 columns
head(d1[, 1:7])
```

## 3. Data Preparation and Preprocessing

### Split the dataset into training and validation?

The dataset is ready. The first step is to split it into training(80%) and test(20%) datasets using caret’s createDataPartition function. The advantage of using createDataPartition() over the traditional random sample() is, it preserves the proportion of the categories in Y variable, that can be disturbed if you sample randomly. Let me quickly refresh why are splitting the dataset into training and test data. The reason is when building models the algorithm should see only the training data to learn the relationship between X and Y. This learned information forms what is called a machine learning model. The model is then used to predict the Y in test data by looking only the X values of test data. Finally, the predicted values of Y is compared to the known Y from test dataset to evaluate how good the model really is. Alright, let?s create the training and test datasets.

```{r}
# Create the training and test datasets
set.seed(100)

# Step 1: Get row numbers for the training data
d1$Ftype=as.factor(d1$Ftype)
trainRowNumbers <- createDataPartition(d1$Ftype, p=0.8, list=FALSE)

# Step 2: Create the training  dataset
trainData <- d1[trainRowNumbers,1:7]

# Step 3: Create the test dataset
testData <- d1[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[, 1:6]
y = trainData$Ftype
```

## Descriptive statistics

Before moving to missing value imputation and feature preprocessing, let’s observe the descriptive statistics of each column in the training dataset. The skimr package provides a nice solution to show key descriptive stats for each column. The `skimr::skim_to_wide()` produces a nice dataframe containing the descriptive stats of each of the columns. The dataframe output includes a nice histogram drawn without any plotting help.

```{r}
library(skimr)
skimmed <- skim(trainData)
skimmed[, c(1:7)]
```

## Create One-Hot Encoding (dummy variables)

Let me first explain what is one-hot encoding and why it is required. Suppose if you have a categorical column as one of the features, it needs to be converted to numeric in order for it to be used by the machine learning algorithms. Just replacing the categories with a number may not be meaningful especially if there is no intrinsic ordering amongst the categories. So what you can do instead is to convert the categorical variable with as many binary (1 or 0) variables as there are categories. An important aspect you should be careful about here is, in real-world environments, you might get new values of categorical variables in the new scoring data. So, you should ensure the dummyVars model is built on the training data alone and that model is in turn used to create the dummy vars on the test data.

In caret, one-hot-encodings can be created using dummyVars(). Just pass in all the features to dummyVars() as the training data and all the factor columns will automatically be converted to one-hot-encodings.

```{r}
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
#trainData$Fault_Cat=as.factor(trainData$Fault_Cat)
dummies_model <- dummyVars(Ftype ~ ., data=trainData)

# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)

# # Convert to dataframe
trainData <- data.frame(trainData_mat)

# # See the structure of the new dataset
str(trainData)
```

## preprocess to transform the data

With the missing values handled and the factors one-hot-encoded, our training dataset is now ready to undergo variable transformations if required. So what type of preprocessing are available in caret?

```{r}
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)

# Append the Y variable
trainData$Ftype <- y

apply(trainData[, 1:7], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
```

## visualize the importance of variables using featurePlot()

Now that the preprocessing is complete, let’s visually examine how the predictors influence the Y (Purchase). In this problem, the X variables are numeric whereas the Y is categorical. So how to gauge if a given X is an important predictor of Y? A simple common sense approach is, if you group the X variable by the categories of Y, a significant mean shift amongst the X’s groups is a strong indicator (if not the only indicator) that X will have a significant role to help predict Y. It is possible to watch this shift visually using box plots and density plots. In fact, caret’s featurePlot() function makes it so convenient. Simply set the X and Y parameters and set plot='box'. You can additionally adjust the label font size (using strip) and the scales to be free as I have done in the below plot.

```{r}
featurePlot(x = trainData[, 1:6], 
            y = trainData$Ftype, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```


```{r}
featurePlot(x = trainData[, 1:6], 
            y = trainData$Ftype, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))
```

##feature selection using recursive feature elimination (rfe)

Most machine learning algorithms are able to determine what features are important to predict the Y. But in some scenarios, you might be need to be careful to include only variables that may be significantly important and makes strong business sense. This is quite common in banking, economics and financial institutions. Or you might just be doing an exploratory analysis to determine important predictors and report it as a metric in your analytics dashboard. Or if you are using a traditional algorithm like like linear or logistic regression, determining what variable to feed to the model is in the hands of the practitioner. Given such requirements, you might need a rigorous way to determine the important variables first before feeding them to the ML algorithm. A good choice of selecting the important features is the recursive feature elimination (RFE). So how does recursive feature elimination work? RFE works in 3 broad steps: Step 1: Build a ML model on a training dataset and estimate the feature importances on the test dataset. Step 2: Keeping priority to the most important variables, iterate through by building models of given subset sizes, that is, subgroups of most important predictors determined from step 1. Ranking of the predictors is recalculated in each iteration. Step 3: The model performances are compared across different subset sizes to arrive at the optimal number and list of final predictors. It can be implemented using the rfe() function and you have the flexibility to control what algorithm rfe uses and how it cross validates by defining the rfeControl().

```{r}
set.seed(100)
options(warn=-1)

subsets <- c(1:6)

ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x=trainData[, 1:6], y=trainData$Ftype,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile
```

## Machine Learning Methods

List of machine learning algorithms defined in the `caret` package is shown below.

```{r}
# See available algorithms in caret
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
```
Each of those is a machine learning algorithm caret supports. Yes, it’s a huge list! And if you want to know more details like the hyperparameters and if it can be used of regression or classification problem, then do a modelLookup(algo). Once you have chosen an algorithm, building the model is fairly easy using the train() function. Let’s train a Multivariate Adaptive Regression Splines (MARS) model by setting the method='earth'. The MARS algorithm was named as ‘earth’ in R because of a possible trademark conflict with Salford Systems. May be a rumor. Or not.

```{r}
modelLookup('glm')
```


### Logistic regression

```{r}
library(nnet)
```


```{r}
# Set the seed for reproducibility
set.seed(100)
glm.fit=multinom(Ftype~., data=trainData)
#predict(glm.fit, testData, "probs")
```

```{r}
summary(glm.fit)
```


## Model measures on training data

```{r}
# Make predictions
predicted.classes <- glm.fit %>% predict(trainData)
head(predicted.classes)
# Model accuracy
mean(predicted.classes == trainData$Ftype)
```

```{r}
#Creating confusion matrix
example <- confusionMatrix(data=trainData$Ftype, reference = predicted.classes)

#Display results 
example
```
## Model measures on test data

```{r}
# Step 1: Impute missing values 
testData2 <- predict(preProcess_missingdata_model, testData)  

# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)

# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)

# View
head(testData4[, 1:6])
```

```{r}
# Make predictions
predicted.classes <- glm.fit %>% predict(testData)
head(predicted.classes)
# Model accuracy
mean(predicted.classes == testData$Ftype)
```

```{r}
#Creating confusion matrix
example <- confusionMatrix(data=testData$Ftype, reference = predicted.classes)

#Display results 
example
```

```{r}
varimp_mars <- varImp(glm.fit)
plot(varimp_mars, main="Variable Importance with GLM")
```


## Using Random Forest Algorithm

```{r}
library(randomForest)
set.seed(100)

# Train the model using rf
model_rf = train(Ftype ~ ., data=trainData, method='rf', tuneLength=5)
model_rf
```

```{r}
# Compute the confusion matrix
confusionMatrix(reference = testData$Ftype, data = model_rf, mode='everything')
```

## Training xgBoost Dart


```{r}
set.seed(100)

# Train the model using extreem gradient boost
model_xgbDART = train(Ftype ~ ., data=trainData, method='xgbDART', tuneLength=6, verbose=F)
model_xgbDART
```
```{r}
varimp_mars <- varImp(model_rf)
plot(varimp_mars, main="Variable Importance with random forest")
```
## SVM method

```{r}
set.seed(100)

# Train the model using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=6)
model_svmRadial
```